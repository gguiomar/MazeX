\documentclass{article}

\usepackage[nonatbib, final]{neurips}

\makeatletter
\renewcommand{\@noticestring}{
  \centering
  
}
\makeatother

\input{extra_pkgs}

\title{Feature Learning with Reward Sensitive Representations}

\author{
  GonÃ§alo Guiomar \\
  %Department of Computer Science \\
  %University of Toronto \\
  %\href{mailto:bojian@cs.toronto.edu}{bojian@cs.toronto.edu}
  \And
  Tiago Costa
  \And
  Margarida Sousa
}

\begin{document}

\maketitle

\begin{abstract}
  ...
\end{abstract}

\section{Introduction}

Basic points to put here:

\begin{itemize}
  \item Exploration of high-dimensional state-spaces is costly
  \item Having a compact representation is necessary to efficiently transverse environments
  \item Reward signals in nature are not symmetric in frequency of occurrence: positive rewards are sparse and localized and negative rewards are diffused
  \item These signals are also contextual; animals seem to bootstrap policies of new environments on what has been learned in other environments
  \item Besides, the brain processes positive and negative reward information through separate pathways
  \item How can we combine this into a more efficient RL algorithm?
  \item We'll attempt to define a reward and context sensitive algorithm that learns policies over a set of environments
  \item We combine graph representation learning techniques with reward sensitivity functions in order to speed up learning across different MDPs
\end{itemize}

\section{Background}
% tano & dayan sensitivity 

\section{Learning the State Embeddings}

% node2vec better than laplacian and other embedding methods
% laura toni and doina precup
% only spatial, no action related information
% might try to do for state-action value function after


\section{Reward Sensitive Representations}
% tano & dayan sensitivity 

\subsection{Context-dependent State-Value Functions}

% take some motivation from state2vec - why is this better etc
% justify in the background section 
As agents experience environments, the probability of observing novel states 
is conditioned on the underlying Markov Decision Process (MDP). The observation
of certain states is thus contextual to the observation of past and future states.

We begin by defining a state value function that takes into account the proximity 
of the surrounding states as proxy for calculating the values; in the usual Function Approximation
formalist [REF-Sutton] the state-value function is given as a linear sum over feature vectors i.e.

\begin{equation}
  V(s_t) = \sum_i w_i \phi_i(s_t)
\end{equation}

where $\phi_i$ is the i-th component of the feature vector corresponding to state $s_t$.

Although feature vectors allow for the compression of state relevant information in a way that 
allows for very high-dimensional state-spaces to be explored, we lose a bit of a notion of context
for each state; to which extent is the state that I'm in related with states that have a similar representation? 
and furthermore, to which extent should the information gathered in surrounding states affect the representation of the current state.

To explore this idea we define a novel value function as dependent on the dot product of the state embeddings in 
a surrounding ball of size $\epsilon$ given by a weight metric function $\Gamma^\epsilon(s_t)$ so that the 
state value function is now given by 

\begin{equation}
  V^h(s_t) = \sum_{i} \Gamma^\epsilon(s_t)_i \langle\phi_i,\mathbf{w}^h\rangle
\end{equation}

with $\langle\phi_i,\mathbf{w}^h\rangle$ the inner product between the weights $\mathbf{w}$ and the embeddings of each state $\phi_i$

\begin{equation}
\Gamma^\epsilon(s_t)_i = 
\begin{cases}
    0 & \text{if } ||\phi(s_t)-\phi_i||>\epsilon\\
     \frac{1}{1+\beta_V ||\phi(s_t)-\phi_i||},         & \text{otherwise}
\end{cases}
\end{equation}

We'll be trying to minimize the Reward Prediction Error (RPE) with rewards filtered by a function $f^h(\delta_t)$ i.e.

\begin{equation}
  \delta^h_t = f^h(r_t) + \gamma V^h(s_{t+1})-V^h(s_t)
\end{equation}

and the loss function for each RPE function is given by

\begin{equation}
  \mathcal{L}^h = ||\delta^h_t||^2
\end{equation}

We can learn the weights $\mathbf{w}$ by the usual gradient descent rule

\begin{equation}
  \frac{\partial \mathcal{L^h}}{\partial w^h_j} = 2 \delta^h_t\frac{\partial \delta^h_t}{\partial w^h_j}
\end{equation}

where the latter partial derivative term can be expanded thus 

\begin{equation}
  \frac{\partial \delta_t}{\partial w^h_j} = \gamma\frac{\partial V^h(s_{t+1})}{\partial w^h_j} - \frac{\partial V^h(s_{t})}{\partial w^h_j} =   \sum_{i} \big ( \gamma \Gamma^\epsilon(s_{t+1})_i \phi_i-\Gamma^\epsilon(s_t)_i\phi_i \big )
\end{equation}

which will give us a final update rule for each weight $w_j$ 

\begin{equation}
  w_j \leftarrow w_j + \alpha^h 2 \delta^h_t\sum_{i} \big ( \gamma \Gamma^\epsilon(s_{t+1})_i \phi_i-\Gamma^\epsilon(s_t)_i\phi_i \big )
\end{equation}

\subsection{Policy}

As a first approach we can try to model the policy with an action preference formalism that takes the RPEs of each channel $h$
and feeds it into a corresponding preference 

\begin{equation}
  A^h(s,a) \leftarrow A^h(s,a) + \alpha \delta^h_t
\end{equation}

where we sum all channels

\begin{equation}
  A^T(s,a) = \sum_h \theta_h A^h(s,a)
\end{equation}

and define our policy through a softmax over the total

\begin{equation}
  \pi(a|s, \theta) = \frac{e^{\sum_h A^T(s,a)}}{\sum_a A^T(s,a)}
\end{equation}

\section{Experiments}

\section{Conclusion}

% \bibliographystyle{ACM-Reference-Format}
% \bibliography{bibliography}

\end{document}
