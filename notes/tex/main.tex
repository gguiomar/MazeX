\documentclass{article}

\usepackage[nonatbib, final]{neurips}

\makeatletter
\renewcommand{\@noticestring}{
  \centering
  
}
\makeatother

\input{extra_pkgs}

\title{Contextual Learning with Reward Sensitive Representations}

\author{
  GonÃ§alo Guiomar \\
  %Department of Computer Science \\
  %University of Toronto \\
  %\href{mailto:bojian@cs.toronto.edu}{bojian@cs.toronto.edu}
  \And
  Tiago Costa
  \And
  Margarida Sousa
}

\begin{document}

\maketitle

\begin{abstract}
  ...
\end{abstract}

\section{Introduction}

Basic points to put here:

\begin{itemize}
  \item Exploration of high-dimensional state-spaces is costly
  \item Having a compact representation is necessary to efficiently transverse environments
  \item Reward signals in nature are not symmetric in frequency of occurrence: positive rewards are sparse and localized and negative rewards are diffused
  \item These signals are also contextual; animals seem to bootstrap policies of new environments on what has been learned in other environments
  \item Besides, the brain processes positive and negative reward information through separate pathways
  \item How can we combine this into a more efficient RL algorithm?
  \item We'll attempt to define a reward and context sensitive algorithm that learns policies over a set of environments
  \item We combine graph representation learning techniques with reward sensitivity functions in order to speed up learning across different MDPs
\end{itemize}

\section{Background}
% tano & dayan sensitivity 

\section{Feature Representations over Graphs}

% node2vec better than laplacian and other embedding methods
% laura toni and doina precup
% only spatial, no action related information
% might try to do for state-action value function after

\newpage
\section{Contextual Learning}
% tano & dayan sensitivity 
% remember that context is not only space but also reward valence information!

We begin by defining a state value function that takes into account the sensitivity to certain 
reward ranges, as in [REF-Tano] we write 

\begin{equation}
  V^h(s_t) = \mathbf{E}^\pi \big[  \sum_\tau^\infty \gamma^\tau f^h(r_{t+\tau}) \big ]
\end{equation}

Although feature vectors allow for the compression of state relevant information in a way that 
allows for very high-dimensional state-spaces to be explored, we lose a bit of a notion of context
for each state; to which extent is the state that the agent is in at time $t$ is related contextually 
with states that have a similar representation? 

To explore this idea we define a novel value function as dependent on the dot product of the state embeddings in 
a surrounding ball of size $\epsilon$ by weighting the surrounding weights with a function $\Gamma^\epsilon(s_t)$


\begin{equation}
  \hat{V}^h(s_t) = \sum_{i} \Gamma^\epsilon(s_t)_i \langle\phi_i,\mathbf{w}^h\rangle
\end{equation}

with $\langle\phi_i,\mathbf{w}^h\rangle$ the inner product between the weights $\mathbf{w}$ and the embeddings of each state $\phi_i$

\begin{equation}
\Gamma^\epsilon(s_t)_i = 
\begin{cases}
    0 & \text{if } ||\phi(s_t)-\phi_i||>\epsilon\\
     \frac{1}{1+\beta_V ||\phi(s_t)-\phi_i||},         & \text{otherwise}
\end{cases}
\end{equation}

and the loss function for each RPE function is given by

\begin{equation}
  \mathcal{L}^h = ||V^h(s_t) - \hat{V}^h(s_t)||^2
\end{equation}

which if we take the following expansion

\begin{equation}
  V^h(s_t) = \mathbf{E}^\pi \big[ f^h(r_{t}) + \gamma \sum_{\tau} \gamma^{\tau} f^h(r_{t+1+\tau})\big ] = f^h(r_t) + V^h(s_{t+1})
\end{equation}

allows us to write the following objective function

\begin{equation}
  \mathcal{L}^h = ||f^h(r_t) + \gamma V^h(s_{t+1}) - \hat{V}^h(s_t)||^2.
\end{equation}

which implies new definition for the RPE 

\begin{equation}
  \delta^h_t = f^h(r_t) + \gamma V^h(s_{t+1})-\hat{V}^h(s_t).
\end{equation}

We can learn the weights $\mathbf{w}$ by the usual gradient descent rule

\begin{equation}
  \frac{\partial \mathcal{L}^h}{\partial w^h_j} = 2 \delta^h_t\frac{\partial \delta^h_t}{\partial w^h_j}
\end{equation}

where in calculating the latter partial derivative we need to take into account that only the approximated 
value function can be differentiated in respect to $\mathbf{w}$ so

\begin{equation}
  \frac{\partial \delta^h_t}{\partial w^h_j} = - \frac{\partial \hat{V}^h(s_{t})}{\partial w^h_j} =   -\sum_{i} \Gamma^\epsilon(s_t)_i\phi_i
\end{equation}

which will give us a final update rule for each weight $w_j$ 

\begin{equation}
  w_j \leftarrow w_j - \alpha^h 2 \delta^h_t\sum_{i}\Gamma^\epsilon(s_t)_i\phi_i
\end{equation}

\subsection{Policy}

As a first approach we can try to model the policy with an action preference formalism that takes the RPEs of each channel $h$
and feeds it into a corresponding preference 

\begin{equation}
  A^h(s,a) \leftarrow A^h(s,a) + \alpha \delta^h_t
\end{equation}

where we sum all channels

\begin{equation}
  A^T(s,a) = \sum_h \theta_h A^h(s,a)
\end{equation}

and define our policy through a softmax over the total

\begin{equation}
  \pi(a|s, \theta) = \frac{e^{\sum_h A^T(s,a)}}{\sum_a A^T(s,a)}
\end{equation}

\section{Experiments}

\section{Conclusion}

% \bibliographystyle{ACM-Reference-Format}
% \bibliography{bibliography}

\end{document}
