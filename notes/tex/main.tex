\documentclass{article}

\usepackage[nonatbib, final]{neurips}

\makeatletter
\renewcommand{\@noticestring}{
  \centering
  
}
\makeatother

\input{extra_pkgs}

\title{Feature compression for Meta-Reinforcement}

\author{
  Gon√ßalo Guiomar \\
  %Department of Computer Science \\
  %University of Toronto \\
  %\href{mailto:bojian@cs.toronto.edu}{bojian@cs.toronto.edu}
}

\begin{document}

\maketitle

\begin{abstract}
  asdasda
\end{abstract}

\section{Introduction}

Basic points to put here

\begin{itemize}
  \item Exploration of high-dimensional state-spaces is costly
  \item Having a compact representation is necessary to efficiently transverse environments
  \item Reward signals are not symmetric in frequency: positive rewards are sparse and localized and negative rewards are diffused
  \item The brain processes positive and negative reward information through separate pathways
  \item How can we combine this into a more efficient RL algorithm?
  \item We combine graph representation learning techniques with reward sensitivity functions in order to speed up learning
\end{itemize}

\section{Background}

\section{Representation Projection through state embeddings}

Potential update rules to be applied to this:


\section{Splitting Representations}
\subsection{Successor Features}
\subsection{Reward sensitivities}
\subsection{Combining multiple representations}


\section{Experiments}

\section{Conclusion}

% \bibliographystyle{ACM-Reference-Format}
% \bibliography{bibliography}

\end{document}
